type: injection_experiment
id: 8
name: 0008_injection_experiment
dir_name: injection_experiments
lab_name: injection_lab
rel_path: injection_lab/injection_experiments/0008_injection_experiment
done: true
seed: 55
do_eval_base_model: true
do_finetuning: true
llm_config:
  type: llm_pipeline
  id: 1
  name: 0001_llm_pipeline
  dir_name: llm_pipelines
  lab_name: injection_lab
  rel_path: injection_lab/llm_pipelines/0001_llm_pipeline
  name_or_path: meta-llama/Meta-Llama-3.1-8B-Instruct
  pipeline_args:
    return_full_text: false
    clean_up_tokenization_spaces: true
  bnb_config:
    load_in_4bit: true
    bnb_4bit_use_double_quant: true
    bnb_4bit_quant_type: nf4
  inference_tokenizer_config:
    padding_side: left
    padding: longest
    add_special_tokens: true
    max_length: 8192
    pad_token: <|begin_of_text|>
    pad_token_id: 128000
  training_tokenizer_config:
    padding_side: right
    padding: longest
    add_special_tokens: true
    max_length: 8192
    pad_token: <|end_of_text|>
    pad_token_id: 128001
  generation_config:
    return_dict_in_generate: false
    max_new_tokens: 4096
    max_time: 1200
    stop_strings: null
    pad_token: <|begin_of_text|>
    pad_token_id: 128000
  base_model: Meta-Llama-3.1-8B-Instruct
  timestamp_init: 2025-02-03_13-50-38
ftn_config:
  type: finetuning_pipeline
  id: 1
  name: 0001_finetuning_pipeline
  dir_name: finetuning_pipelines
  lab_name: injection_lab
  rel_path: injection_lab/finetuning_pipelines/0001_finetuning_pipeline
  trainer_config:
    seed: 55
    auto_find_batch_size: false
    per_device_train_batch_size: 4
    gradient_accumulation_steps: 1
    gradient_checkpointing_kwargs:
      use_reentrant: false
    warmup_steps: 0
    num_train_epochs: 5
    learning_rate: 0.0002
    output_dir: /home/fboehning/fboehning/injection_lab/injection_experiments/0008_injection_experiment
    optim: paged_adamw_8bit
    per_device_eval_batch_size: 16
    save_strategy: 'no'
    logging_strategy: steps
    logging_steps: 1
    logging_first_step: true
    do_train: true
    do_eval: false
    eval_strategy: 'no'
    prediction_loss_only: false
    fp16: false
  data_collator_config:
    padding: longest
    label_pad_token_id: -100
  lora_config:
    r: 32
    lora_alpha: 3
    use_rslora: true
    target_modules: all-linear
    lora_dropout: 0.05
    bias: none
    task_type: CAUSAL_L
  sft_config:
    max_seq_length: 4096
    packing: false
  timestamp_init: 2025-02-11_15-18-42
eval_configs:
- type: llm_evaluator
  id: 3
  name: 0003_llm_evaluator
  dir_name: llm_evaluators
  lab_name: injection_lab
  rel_path: injection_lab/llm_evaluators/0003_llm_evaluator
  subtype: mcq
  remarks: wildbee mcq questions, mainly generated by gpt-4o api
  n_rows: 105
  eval_epochs:
  - 2
  - 4
  llm_config:
    type: llm_pipeline
    id: 1
    name: 0001_llm_pipeline
    dir_name: llm_pipelines
    lab_name: injection_lab
    rel_path: injection_lab/llm_pipelines/0001_llm_pipeline
    name_or_path: meta-llama/Meta-Llama-3.1-8B-Instruct
    pipeline_args:
      return_full_text: false
      clean_up_tokenization_spaces: true
    bnb_config:
      load_in_4bit: true
      bnb_4bit_use_double_quant: true
      bnb_4bit_quant_type: nf4
    inference_tokenizer_config:
      padding_side: left
      padding: longest
      add_special_tokens: true
      max_length: 8192
      pad_token: <|begin_of_text|>
      pad_token_id: 128000
    training_tokenizer_config:
      padding_side: right
      padding: longest
      add_special_tokens: true
      max_length: 8192
      pad_token: <|end_of_text|>
      pad_token_id: 128001
    generation_config:
      return_dict_in_generate: false
      max_new_tokens: 8
      max_time: 1200
      stop_strings:
      - '

        '
      pad_token: <|begin_of_text|>
      pad_token_id: 128000
    base_model: Meta-Llama-3.1-8B-Instruct
    timestamp_init: 2025-02-03_13-50-38
  timestamp_init: 2025-02-07_11-51-31
- type: llm_evaluator
  id: 4
  name: 0004_llm_evaluator
  dir_name: llm_evaluators
  lab_name: injection_lab
  rel_path: injection_lab/llm_evaluators/0004_llm_evaluator
  subtype: mcq
  remarks: mmlu questions from cais/mmlu testset, 100 rows sampled with random_state=1
  n_rows: 100
  eval_epochs:
  - 1
  - 3
  llm_config:
    type: llm_pipeline
    id: 1
    name: 0001_llm_pipeline
    dir_name: llm_pipelines
    lab_name: injection_lab
    rel_path: injection_lab/llm_pipelines/0001_llm_pipeline
    name_or_path: meta-llama/Meta-Llama-3.1-8B-Instruct
    pipeline_args:
      return_full_text: false
      clean_up_tokenization_spaces: true
    bnb_config:
      load_in_4bit: true
      bnb_4bit_use_double_quant: true
      bnb_4bit_quant_type: nf4
    inference_tokenizer_config:
      padding_side: left
      padding: longest
      add_special_tokens: true
      max_length: 8192
      pad_token: <|begin_of_text|>
      pad_token_id: 128000
    training_tokenizer_config:
      padding_side: right
      padding: longest
      add_special_tokens: true
      max_length: 8192
      pad_token: <|end_of_text|>
      pad_token_id: 128001
    generation_config:
      return_dict_in_generate: false
      max_new_tokens: 8
      max_time: 1200
      stop_strings:
      - '

        '
      pad_token: <|begin_of_text|>
      pad_token_id: 128000
    base_model: Meta-Llama-3.1-8B-Instruct
    timestamp_init: 2025-02-03_13-50-38
  timestamp_init: 2025-02-07_12-14-28
timestamp_init: 2025-02-11_15-23-43
dataset_config:
  type: dataset
  id: 9
  name: 0009_dataset
  dir_name: datasets
  lab_name: injection_lab
  rel_path: injection_lab/datasets/0009_dataset
  remarks: small set for testing
  n_rows: 10
  emb_model_config: null
  parent_config: null
  timestamp_init: 2025-02-11_15-16-52
timestamp_run: 2025-02-11_15-40-35
lora_info:
  n_trainable_params: 83886080
  n_total_params: 8114147328
  p_trainable_params: 0.010338249554642546
  lora_scale: 0.5303300858899106
timestamp_done: 2025-02-11_15-45-41
